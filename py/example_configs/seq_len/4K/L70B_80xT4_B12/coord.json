{
  "config_name": "L70B_80xT4_B12",
  "model_name": "llama2-70b-chat-4k",
  "listen_address": "0.0.0.0",
  "listen_port": "3075",
  "n_layers": 80,
  "n_slices": 80,
  "tiers": [
    {
      "kernel": "batched",
      "platform": "cuda",
      "context": "static",
      "ranks": 1,
      "concurrency_size_pre": 10,
      "concurrency_size_att": 10,
      "concurrency_size_post": 10,
      "concurrency_size_cls": 10,
      "max_context_count": 800,
      "concurrency_size_pre_non_faux": 12,
      "concurrency_size_att_non_faux": 12,
      "concurrency_size_post_non_faux": 12,
      "concurrency_size_cls_non_faux": 12,
      "max_context_count_non_faux": 816,
      "latency": 0
    }
  ],
  "separate_cls_tiers": []
}